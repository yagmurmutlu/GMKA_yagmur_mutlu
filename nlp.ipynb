{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5G5YuDy/lq5FvPzl4F8IU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagmurmutlu/GMKA_yagmur_mutlu/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bspy-65_Ygr1"
      },
      "outputs": [],
      "source": [
        "### kütüphaneleri tanımlayınız. ### \n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "# numerik karakterlerin kaldırılması\n",
        "def remove_numeric(value):\n",
        "    bfr = [item for item in value if not item.isdigit()]\n",
        "    return ''.join(bfr)\n",
        "    \n",
        "\n",
        "# emojilerin kaldırılması\n",
        "def remove_emoji(value):\n",
        "    bfr=re.compile(\"[\\U00010000-\\U0010ffff]\",flags=re.UNICODE)\n",
        "    bfr=bfr.sub(r'',value)\n",
        "    return bfr\n",
        "\n",
        "#noktalama işaretlerinin kaldırılması\n",
        "def remove_noktalama(value):\n",
        "    return re.sub(r'[^\\w\\s]','',value)\n",
        "\n",
        "#tek karakterli ifadelerin kaldırılması\n",
        "import re\n",
        "\n",
        "def remove_single_character(value):\n",
        "    return re.sub(r'\\b\\w\\b', '', value)\n",
        "\n",
        "#linklerin kaldırılması \n",
        "def remove_link(value):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?//[^\\s]+))','',value)\n",
        "\n",
        "# hashtaglerin kaldırılması\n",
        "def remove_hashtag(value):\n",
        "    return re.sub(r'#[^\\s]+','',value)\n",
        "    \n",
        "\n",
        "# kullanıcı adlarının kaldırılması\n",
        "def remove_username(value):\n",
        "    return re.sub('@[^\\s]+','',value)\n",
        "\n",
        "\n",
        "#kök indirgeme ve stop words işlemleri\n",
        "def stem_word(value):\n",
        "    stemmer = SnowballStemmer(\"turkish\")\n",
        "    value = value.lower()\n",
        "    value = stemmer.stemWords(value.split())\n",
        "    stop_words = ['acaba', 'ama', 'aslinda', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu',\n",
        "                  'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hic',\n",
        "                  'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mi', 'mu', 'mü', 'nasil', 'ne', 'neden', 'nerde', 'nerede',\n",
        "                  'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'su', 'tüm', 've', 'veya', 'ya', 'yani',\n",
        "                  'bir', 'iki', 'üç', 'dört', 'beş', 'altı', 'yedi', 'sekiz', 'dokuz', 'on']\n",
        "    value = [item for item in value if not item in stop_words]\n",
        "    value = ' '.join(value)\n",
        "    return value\n",
        "# ön işlem fonksiyonlarının sırayla çağırılması   \n",
        "def pre_processing (value):\n",
        "    return [remove_numeric(remove_emoji\n",
        "            (remove_noktalama\n",
        "             (remove_single_character\n",
        "              (remove_link\n",
        "               (remove_hashtag\n",
        "                (remove_username\n",
        "                 (stem_word(word)))))))) for word in value.split()]\n",
        "\n",
        "# Boşlukların kaldırılması\n",
        "def remove_space(value):\n",
        "    return [item for item in value if item.strip()]\n",
        "\n",
        "# word2vec model oluşturma ve kaydetme\n",
        "def word2vec_create(value):\n",
        "    model = Word2Vec(sentences = value.tolist(),vector_size=100,window=5,min_count=1)\n",
        "    model.save(\"data/word2vec.model\")\n",
        "\n",
        "# word2vec model yükleme ve vektör çıkarma\n",
        "def word2vec_analysis(value):\n",
        "    model = Word2Vec.load(\"/content/data/word2vec.model\")\n",
        "    bfr_list = []\n",
        "    bfr_len = len(value)\n",
        "    for k in value:\n",
        "        bfr=model.wv.key_to_index[k]\n",
        "        bfr=model.wv[bfr]\n",
        "        bfr_list.append(bfr)\n",
        "    bfr_list = sum(bfr_list)\n",
        "    bfr_list=bfr_list/bfr_len \n",
        "    return bfr_list.tolist()\n",
        "\n",
        "# word2vec model güncellenir.\n",
        "def word2vec_update(value):\n",
        "    model = Word2Vec.load(\"/content/data/word2vec.model\")\n",
        "    model.build_vocab(value.tolist(),update=True)\n",
        "    model.save(\"/content/data/word2vec.model\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   \n",
        "    # veri temizlemesi için örnek veri kümemiz okunur.\n",
        "    df_1 = pd.read_csv(\"//content/data/nlp.csv\",index_col=0)\n",
        "\n",
        "\n",
        "    ### tanımlanan df_1 içerisinde Text sütununu ön işlem fonksiyonlarından geçirerek Text_2 olarak df_1 içerisinde yeni bir sütun oluşturun. ###\n",
        "    df_1[\"Text_2\"]=df_1[\"Text\"].apply(pre_processing)\n",
        "    \n",
        "\n",
        "    ### df_1 içerisinde Text_2 sütununda boş liste kontrolü ###\n",
        "    df_1[df_1[\"Text_2\"].str[0].isnull()]\n",
        "    df_1_index = df_1[df_1[\"Text_2\"].str[0].isnull()].index\n",
        "\n",
        "    df_1=df_1.drop(df_1_index)\n",
        "    df_1 = df_1.reset_index() \n",
        "    del df_1[\"index\"] \n",
        "\n",
        "    \n",
        "    ### word2vec model oluşturma ###\n",
        "    \n",
        "    df_1[\"word2vec\"]=df_1[\"Text_2\"].apply(word2vec_analysis)\n",
        "                                      \n",
        "    # df_1 dataframe mizi artık kullanmaycağımızdan ram de yer kaplamaması adına boş bir değer ataması yapıyoruz.\n",
        "    df_1 = {}\n",
        "    # sınıflandırma yapacağımız veri okunur.\n",
        "    df_2 = pd.read_csv(\"/content/data/metin_siniflandirma.csv\",index_col=0)\n",
        "\n",
        "    ### tanımlanan df_2 içerisinde Text sütununu ön işlem fonksiyonlarından geçirerek Text_2 olarak df_2 içerisinde yeni bir sütun oluşturun. ###\n",
        "    df_2[\"Text_2\"]=df[\"Text\"].apply(pre_processing)\n",
        "\n",
        "    ### df_2 içerisinde Text_2 sütununda boş liste kontrolü ###\n",
        "    df_2[df_2[\"Text_2\"].str[0].isnull()]\n",
        "    df_2_index= df_2[df_2(\"Text_2\").str[0].isnull()].index\n",
        "    df_2=df_2.drop(df_2_index)\n",
        "    df_2 = df_2.reset_index() \n",
        "    del df_2[\"index\"] \n",
        "\n",
        "\n",
        "    ### sınıflandırma yapacağımız df_2 içerisinde bulunan Text_2 sütun verisini word2vec verisinde güncelleyin. ### \n",
        "    df_2[\"word2vec\"]=df_2[\"Text_2\"].apply(word2vec_update)\n",
        "    ### Text_2 sütun üzerinden word2vec adında bu modeli kullanarak yeni bir sütun yaratın\n",
        "    df_2[\"word2vec\"]=df_2[\"Text_2\"].apply(word2vec_analysis)\n",
        "    ### word2vec sütunumuzu train test olarak bölün ###\n",
        "    df_2.groupby(\"Label\").size()\n",
        "    msg_train, msg_test, label_train, label_test = train_test_split(df_2[\"word2vec\"].tolist(), df_2[\"Label\"].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "    ### svm pipeline oluştur, modeği eğit ve test et ###\n",
        "    svm = Pipeline([ ('svm', LinearSVC())])\n",
        "    svm.fit(msg_train, label_train)\n",
        "    y_pred_class = svm.predit (msg_test)\n",
        "\n",
        "    \n",
        "    ### accuracy ve f1 score çıktısını print ile gösterin. ###\n",
        "    print(\"svm accuracy score\", accuracy_score (label_test,y_pred_class))\n",
        "    print(\"svm f1 score\", f1_score (label_test, y_pred_class, average=\"weighted\"))\n",
        "                  "
      ]
    }
  ]
}